{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cfc2597-4155-4177-993d-197508aa9d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# folder locations\n",
    "folders = [\n",
    "    \"C:/Users/Nila/Documents/after_shorten_final/BART-CNN/depth_first\",\n",
    "    \"C:/Users/Nila/Documents/after_shorten_final/BART-CNN/divide_conquer\",\n",
    "    \"C:/Users/Nila/Documents/after_shorten_final/GPT-4/depth_first\",\n",
    "    \"C:/Users/Nila/Documents/after_shorten_final/GPT-4/divide_conquer\",\n",
    "    \"C:/Users/Nila/Documents/after_shorten_final/GPT-4/json\",\n",
    "    \"C:/Users/Nila/Documents/after_shorten_final/LLAMA2/depth_first\",\n",
    "    \"C:/Users/Nila/Documents/after_shorten_final/LLAMA2/divide_conquer\",\n",
    "    \"C:/Users/Nila/Documents/after_shorten_final/LLAMA2/json\",\n",
    "]\n",
    "\n",
    "arg_folder = \"C:/Users/Nila/Documents/arg_graphs\"\n",
    "\n",
    "# empty DataFrame\n",
    "df = pd.DataFrame(columns=[\"Model 1\", \"Approach 1\", \"Model 2\", \"Approach 2\", \"Filename\", \"Summary 1\", \"Summary 2\"])\n",
    "\n",
    "# Looping through each file in the first folder\n",
    "first_folder = folders[0]\n",
    "df_list = []\n",
    "\n",
    "for filename in os.listdir(first_folder):\n",
    "    \n",
    "    #lists to store information for each summary\n",
    "    model_list = []\n",
    "    approach_list = []\n",
    "    summaries = []\n",
    "    source_arg = []\n",
    "\n",
    "    # Read information and summaries for each filename from all folders\n",
    "    for folder in folders:\n",
    "        folder_split = folder.split(\"/\")\n",
    "        model = folder_split[-2]\n",
    "        approach = folder_split[-1]\n",
    "        model_list.append(model)\n",
    "        approach_list.append(approach)\n",
    "\n",
    "        file_path = os.path.join(folder, filename)\n",
    "        #print(file_path)\n",
    "\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            content = file.read()\n",
    "            summaries.append(content)\n",
    "      \n",
    "    # Generate pairs for the summaries\n",
    "    # only unique pairs will be created\n",
    "    for i in range(len(summaries)):\n",
    "        for j in range(i + 1, len(summaries)):\n",
    "            model1 = model_list[i]\n",
    "            approach1 = approach_list[i]\n",
    "            model2 = model_list[j]\n",
    "            approach2 = approach_list[j]\n",
    "\n",
    "            summary1 = summaries[i]\n",
    "            summary2 = summaries[j]\n",
    "            \n",
    "            # Removing the extension and split by hyphen\n",
    "            filename_without_extension = os.path.splitext(filename)[0]\n",
    "            parts = filename_without_extension.split('-')\n",
    "            updated_filename = ' '.join(parts[:-1])\n",
    "            question = f'Consider the summaries related to the topic \"{updated_filename}\". Please choose the summary that you think has the best \"overall quality\". Factors such as relevance, coherence, and fluency can be considered when making your choice.'\n",
    "\n",
    "            # Add the argument content.\n",
    "            arg_file_path = os.path.join(arg_folder, filename)\n",
    "\n",
    "            with open(arg_file_path, \"r\", encoding=\"utf-8\") as arg_file:\n",
    "                arg_content = arg_file.read()\n",
    "\n",
    "            #get the filename without extension\n",
    "            filename_without_extension = os.path.splitext(filename)[0]\n",
    "            arg_link = \"www.kialo.com/\" + filename_without_extension\n",
    "\n",
    "            # Create a DataFrame\n",
    "            pair_df = pd.DataFrame({\n",
    "                \"ArgGraph\": [arg_link],\n",
    "                \"Model 1\": [model1],\n",
    "                \"Approach 1\": [approach1],\n",
    "                \"Model 2\": [model2],\n",
    "                \"Approach 2\": [approach2],\n",
    "                \"Question\": [question],\n",
    "                \"Summary 1\": [summary1],\n",
    "                \"Summary 2\": [summary2],\n",
    "                \"Source\": [arg_content],\n",
    "            })\n",
    "\n",
    "\n",
    "            # Append the DataFrame to the list\n",
    "            df_list.append(pair_df)\n",
    "\n",
    "            # Concatenate all DataFrames in the list\n",
    "            final_df = pd.concat(df_list, ignore_index=True)\n",
    "            # write dataframe to excel\n",
    "            final_df.to_excel(\"all_questions_8feb16.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d51b0a8a-9994-4da3-9632-8b27bcb8fc4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: openpyxl in c:\\users\\nila\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.1.2)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\nila\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openpyxl) (1.1.0)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install openpyxl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6418fc2-47c4-4934-84b1-9a364cd221a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "questions_df = pd.read_excel('all_questions_8feb16.xlsx')\n",
    "\n",
    "# Shuffle the rows\n",
    "shuffle_df = questions_df.sample(frac=1.0, random_state=52)\n",
    "\n",
    "shuffle_df.insert(0, 'Index', range(1, len(shuffle_df) + 1))\n",
    "\n",
    "shuffle_df.to_excel('shuffled_file_8feb16.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03e30165-423a-4c0e-883b-3ada1f2d2ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the shuffled file into multiple batches\n",
    "\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "df = pd.read_excel('shuffled_file_8feb16.xlsx')\n",
    "\n",
    "# Calculate the number of splits needed\n",
    "# round off\n",
    "total_record = 1400\n",
    "chunk_size = 250\n",
    "num_splits = math.ceil(total_record / chunk_size)\n",
    "\n",
    "# Split the initial data into chunks of 250 records each and write them to separate files\n",
    "for i in range(num_splits):\n",
    "    start_idx = i * chunk_size\n",
    "    end_idx = min((i + 1) * chunk_size, total_record)\n",
    "    \n",
    "    # Create a subset\n",
    "    subset = df.iloc[start_idx:end_idx]\n",
    "    \n",
    "    output_file = f'summaries_batch{i + 1}.xlsx'\n",
    "\n",
    "    output_folder = f'./batches/{output_file}'\n",
    "    \n",
    "    # Write the result\n",
    "    subset.to_excel(output_folder, index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58a3f82-34ab-44e9-a073-588f33ce7c42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d849cd5-271d-4de9-9b73-9d5c1a25e17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4 turbo summarization for dfs input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1c3d8a8-af18-4638-8152-78a906a56527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai==0.28 in c:\\users\\nila\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.28.0)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\nila\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai==0.28) (2.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\nila\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai==0.28) (4.66.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\nila\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai==0.28) (3.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nila\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nila\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.20->openai==0.28) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nila\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.20->openai==0.28) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nila\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.20->openai==0.28) (2023.11.17)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\nila\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->openai==0.28) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\nila\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->openai==0.28) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\nila\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->openai==0.28) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\nila\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->openai==0.28) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\nila\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->openai==0.28) (1.3.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\nila\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm->openai==0.28) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install openai==0.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "570102d2-4aed-4521-b2df-50767f08f04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dfb5b9c-3c55-4e18-be84-2052ea08a7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = \"anything\"\n",
    "openai.api_base = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea893e4d-6b03-42f7-9506-837825f64c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(Messages):\n",
    "    response = openai.ChatCompletion.create(\n",
    "    model=\"ollama/llama2:70b\",\n",
    "    messages=Messages\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2da72520-431d-40fc-84fb-c61aeeafd95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# function to porcess dfs input and create llama2 summaries\n",
    "def create_llama2_summaries(input_folder, output_folder, max_tokens):\n",
    "\n",
    "    # Iterate through files in the input folder\n",
    "    for filename in os.listdir(input_folder):\n",
    "        chunks = []\n",
    "        if filename.endswith(\".txt\"):\n",
    "            print(filename)\n",
    "            input_file_path = os.path.join(input_folder, filename)\n",
    "\n",
    "            # read the content of the file\n",
    "            with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "                file_content = file.read()\n",
    "\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-70b-chat-hf\")\n",
    "\n",
    "            token_integers = tokenizer.encode(file_content)\n",
    "            \n",
    "            chunksize = max_tokens\n",
    "            chunks = [\n",
    "                token_integers[i : i + chunksize]\n",
    "                for i in range(0, len(token_integers), chunksize)\n",
    "            ]\n",
    "            \n",
    "            chunks = [tokenizer.decode(chunk) for chunk in chunks]\n",
    "            \n",
    "            prev_summary = \"\"\n",
    "            total_tokens_used = 0\n",
    "            tokens_used = 0\n",
    "            responses = []\n",
    "            \n",
    "            for chunk in chunks:\n",
    "                # Combine previous summary and current chunk for summarization\n",
    "                summary_chunk =  prev_summary + chunk  \n",
    "\n",
    "                #print('summary_chunk',summary_chunk)\n",
    "                \n",
    "                Messages = [\n",
    "                        {\"role\": \"system\", \"content\": \"You are a text summarizer. your task is to generate a concise summary of the provided debate within the strict 90-word limit without making assumptions or utilizing bullet points.\"},\n",
    "                        {\"role\": \"user\", \"content\": summary_chunk }\n",
    "                    ]\n",
    "            \n",
    "                final_summary = chat(Messages)\n",
    "                #print('final_summary',final_summary)\n",
    "\n",
    "                # Update previous summary for the next iteration\n",
    "                prev_summary = final_summary  \n",
    "            \n",
    "                responses.append(final_summary)\n",
    "                \n",
    "            # write the concatenated result to a text file in the output folder\n",
    "            output_file_path = os.path.join(output_folder, filename)\n",
    "            with open(output_file_path, 'w', encoding=\"utf8\") as output_file:\n",
    "                output_file.write(final_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00d4535e-9645-4e1d-906e-d9ab4faea73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3d-printer-and-guns-should-blueprints-of-3d-printed-weapons-be-prohibited-17593.txt\n",
      "a-bar-of-soap-is-better-than-a-bottle-of-shower-gel-21205.txt\n",
      "a-childs-primary-carer-should-receive-a-wage-until-the-child-enters-primary-school-or-some-other-form-of-care-17763.txt\n",
      "a-flat-asset-tax-is-all-the-tax-we-should-ever-pay-16974.txt\n",
      "a-free-press-is-necessary-to-democracy-8559.txt\n",
      "a-permanent-venue-for-the-olympic-games-1335.txt\n",
      "addressing-psychosocial-factors-is-essential-to-reducing-or-preventing-school-shootings-11784.txt\n",
      "agile-certifications-do-not-advance-agile-thinking-14073.txt\n",
      "air-traffic-control-atc-should-not-be-privatized-12138.txt\n",
      "alex-jones-has-a-negative-impact-on-society-19233.txt\n",
      "all-cars-should-be-run-at-high-revs-sometimes-to-prevent-carbon-build-up-9119.txt\n",
      "all-children-should-be-taught-to-code-in-school-6844.txt\n",
      "all-dating-sites-should-include-background-checks-17733.txt\n",
      "all-drugs-should-be-legalized-7100.txt\n",
      "all-human-interaction-should-be-voluntary-6835.txt\n",
      "all-transgender-athletes-should-have-to-compete-in-mens-divisions-21194.txt\n",
      "all-us-and-eu-sanctions-imposed-on-russia-since-2014-should-be-lifted-25506.txt\n",
      "american-civil-war-1861-65-was-mainly-about-slavery-19208.txt\n",
      "american-football-should-be-banned-10143.txt\n",
      "anarchy-is-the-only-ethical-system-of-society-6144.txt\n",
      "are-all-our-actions-selfish-to-some-degree-31063.txt\n",
      "are-bitcoin-and-similar-cryptocurrencies-good-for-the-world-333.txt\n",
      "are-canadas-new-impaired-driving-laws-appropriate-18375.txt\n",
      "are-cats-more-preferred-by-humans-than-dogs-31829.txt\n",
      "are-constructed-languages-useful-and-do-we-need-more-of-them-16548.txt\n",
      "are-countries-right-to-dispel-russian-diplomats-over-the-skripal-poisoning-case-12134.txt\n",
      "are-crop-circles-created-by-extraterrestials-19554.txt\n",
      "are-decentralized-marketplaces-like-open-bazaar-antithetical-to-privacy-preserving-trustless-marketplaces-18191.txt\n",
      "are-exorbitant-transfer-fees-in-football-soccer-unethical-5096.txt\n",
      "are-free-markets-better-for-humans-than-regulated-markets-7841.txt\n",
      "are-gender-and-sex-the-same-thing-30703.txt\n",
      "are-ghosts-real-23956.txt\n",
      "are-gmos-the-solution-to-limited-means-14152.txt\n",
      "are-homeopathic-remedies-fraudulent-8668.txt\n",
      "are-humans-evil-9827.txt\n",
      "are-humans-fundamentally-different-from-other-animals-9265.txt\n",
      "are-humans-primarily-driven-by-emotions-rather-than-rationality-14191.txt\n",
      "are-identity-politics-detrimental-to-society-7018.txt\n",
      "are-k-12-teachers-already-paid-enough-in-america-21844.txt\n",
      "are-killer-drones-an-acceptable-weapon-in-war-17822.txt\n",
      "are-men-the-biggest-problem-mankind-faces-13628.txt\n",
      "are-modern-democracies-destined-to-fail-due-to-their-inherent-weaknesses-26330.txt\n",
      "are-people-who-do-immoral-things-immoral-9202.txt\n",
      "are-problems-in-developing-countries-worse-than-developed-ones-15573.txt\n",
      "are-public-speakers-that-promote-self-help-making-a-difference-or-just-enriching-themselves-22957.txt\n",
      "are-purity-pledges-harmful-29355.txt\n",
      "are-real-estate-brokers-really-necessary-7185.txt\n",
      "are-the-milankovitch-cycles-major-causes-of-climate-change-14194.txt\n",
      "are-the-rules-of-the-criminal-justice-system-fair-15002.txt\n",
      "are-there-any-working-business-models-for-journalism-10310.txt\n",
      "are-unions-a-good-thing-2760.txt\n",
      "are-we-morally-compelled-to-help-others-in-need-18090.txt\n",
      "are-women-better-than-men-26866.txt\n",
      "aritificial-intelligence-ai-limiting-an-ais-freedom-of-thought-is-unethical-15943.txt\n",
      "arranged-marriages-are-better-than-love-matches-16340.txt\n"
     ]
    }
   ],
   "source": [
    "#input_folder_path = \"C:/Users/Nila/Documents/Main Scripts_25.12.2023/Jupyter Notebooks/complete testing/llm_input/dfs_linearized/\"\n",
    "#output_folder_path = \"C:/Users/Nila/Documents/Main Scripts_25.12.2023/Jupyter Notebooks/complete testing/llm_output/GPT4/depth_first/\"\n",
    "\n",
    "input_folder_path = \"C:/Users/Nila/Documents/Main Scripts_25.12.2023/Jupyter Notebooks/complete testing/llm_input/dfs_linearized/\"\n",
    "output_folder_path = \"C:/Users/Nila/Documents/Main Scripts_25.12.2023/Jupyter Notebooks/complete testing/llm_output/LLAMA2/depth_first/\"\n",
    "\n",
    "# setting chunksize with max token limit as 3700, because bart accepts the maximum of 4096 tokens. At each step, it add the summary\n",
    "# from previous chunk, which contains maximum of 100 tokens. The return summary will contain a maximum of 100 tokens. (3700+100+100\n",
    "# = 3900 tokens). It requires some token for internal processing. so, 3700 would be a good chunk size.\n",
    "create_llama2_summaries(input_folder_path, output_folder_path,3700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ab0e1a-7d48-4116-9097-f8ba36f2f022",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
